{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_performance_log(log_file_path):\n",
    "    \"\"\"Parse the performance log and extract stream statistics.\"\"\"\n",
    "    stream_data = defaultdict(list)\n",
    "    gpu_data = []\n",
    "    \n",
    "    with open(log_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_test_streams = None\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            log_entry = json.loads(line.strip())\n",
    "            \n",
    "            # Extract number of streams from initialization messages\n",
    "            if \"Initialized\" in log_entry.get(\"fields\", {}).get(\"message\", \"\"):\n",
    "                match = re.search(r\"Initialized (\\d+) processors\", log_entry[\"fields\"][\"message\"])\n",
    "                if match:\n",
    "                    current_test_streams = int(match.group(1))\n",
    "            \n",
    "            # Extract inference statistics\n",
    "            if log_entry.get(\"target\") == \"client::inference::source\" and \"inference statistics\" in log_entry.get(\"fields\", {}).get(\"message\", \"\"):\n",
    "                if current_test_streams is not None:\n",
    "                    fields = log_entry[\"fields\"]\n",
    "                    stream_data[current_test_streams].append({\n",
    "                        'avg_processing': fields.get('avg_processing', 0),\n",
    "                        'total_success': fields.get('frames_success', 0),\n",
    "                        'total_expected': fields.get('frames_expected', 0),\n",
    "                        'total_failed': fields.get('frames_failed', 0)\n",
    "                    })\n",
    "            \n",
    "            # Extract GPU utilization data\n",
    "            if \"GPU utilization information\" in log_entry.get(\"fields\", {}).get(\"message\", \"\"):\n",
    "                fields = log_entry[\"fields\"]\n",
    "                gpu_data.append({\n",
    "                    'timestamp': log_entry[\"timestamp\"],\n",
    "                    'util_perc': fields.get('util_perc', 0),\n",
    "                    'memory_perc': fields.get('memory_perc', 0),\n",
    "                    'memory_used_mb': fields.get('memory_used_mb', 0),\n",
    "                    'memory_total_mb': fields.get('memory_total_mb', 0),\n",
    "                    'streams': current_test_streams\n",
    "                })\n",
    "                \n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            continue\n",
    "    \n",
    "    return stream_data, gpu_data\n",
    "\n",
    "\n",
    "def calculate_stream_stats(stream_data, gpu_data):\n",
    "    \"\"\"Calculate aggregated statistics for each stream count.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for num_streams in sorted(stream_data.keys()):\n",
    "        # Calculate inference statistics - filter out zero avg_processing values\n",
    "        valid_entries = [entry for entry in stream_data[num_streams] if entry['avg_processing'] > 0]\n",
    "        processing_times = [entry['avg_processing'] for entry in valid_entries]\n",
    "        success_counts = [entry['total_success'] for entry in valid_entries]\n",
    "        expected_counts = [entry['total_expected'] for entry in valid_entries]\n",
    "        failed_counts = [entry['total_failed'] for entry in valid_entries]\n",
    "        \n",
    "        if not processing_times:\n",
    "            continue\n",
    "            \n",
    "        # Convert from microseconds to milliseconds\n",
    "        avg_latency = np.mean(processing_times) / 1000\n",
    "        p99_latency = np.percentile(processing_times, 99) / 1000\n",
    "        min_latency = np.min(processing_times) / 1000\n",
    "        max_latency = np.max(processing_times) / 1000\n",
    "        \n",
    "        # Calculate completion rates - handle cases where success > expected\n",
    "        total_success = sum(success_counts)\n",
    "        total_expected = sum(expected_counts)\n",
    "        total_failed = sum(failed_counts)\n",
    "        \n",
    "        # Use the maximum of success+failed vs expected to avoid >100% rates\n",
    "        total_processed = total_success + total_failed\n",
    "        effective_expected = max(total_expected, total_processed)\n",
    "        \n",
    "        frame_completion_rate = (total_success / max(effective_expected, 1)) * 100 if effective_expected > 0 else 0\n",
    "        frame_miss_rate = (total_failed / max(effective_expected, 1)) * 100 if effective_expected > 0 else 0\n",
    "        \n",
    "        # Calculate average frames completed per expected (capped at expected)\n",
    "        avg_frames_completed = total_success / len(success_counts) if success_counts else 0\n",
    "        avg_frames_expected = total_expected / len(expected_counts) if expected_counts else 0\n",
    "        \n",
    "        # Calculate GPU utilization for this stream count\n",
    "        gpu_utils_for_streams = [entry['util_perc'] for entry in gpu_data if entry['streams'] == num_streams]\n",
    "        gpu_memory_for_streams = [entry['memory_perc'] for entry in gpu_data if entry['streams'] == num_streams]\n",
    "        avg_gpu_util = np.mean(gpu_utils_for_streams) if gpu_utils_for_streams else 0\n",
    "        avg_gpu_memory = np.mean(gpu_memory_for_streams) if gpu_memory_for_streams else 0\n",
    "        \n",
    "        results.append({\n",
    "            'num_streams': num_streams,\n",
    "            'avg_latency_ms': avg_latency,\n",
    "            'p99_latency_ms': p99_latency,\n",
    "            'min_latency_ms': min_latency,\n",
    "            'max_latency_ms': max_latency,\n",
    "            'frame_completion_rate': frame_completion_rate,\n",
    "            'frame_miss_rate': frame_miss_rate,\n",
    "            'gpu_util_percent': avg_gpu_util,\n",
    "            'gpu_memory_percent': avg_gpu_memory,\n",
    "            'avg_frames_completed': avg_frames_completed,\n",
    "            'avg_frames_expected': avg_frames_expected\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_performance_analysis(log_file_path, efficiency_threshold=90, latency_cap_ms=34, gpu_util_cap=90):\n",
    "    \"\"\"Create performance analysis plot from log file.\"\"\"\n",
    "    \n",
    "    # Parse the log file\n",
    "    stream_data, gpu_data = parse_performance_log(log_file_path)\n",
    "    \n",
    "    if not stream_data:\n",
    "        print(\"No stream data found in log file\")\n",
    "        return\n",
    "    \n",
    "    # Calculate statistics\n",
    "    results = calculate_stream_stats(stream_data, gpu_data)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid results calculated\")\n",
    "        return\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    x_streams = [r['num_streams'] for r in results]\n",
    "    y_latency = [r['avg_latency_ms'] for r in results]\n",
    "    gpu_utils = [r['gpu_util_percent'] for r in results]\n",
    "    gpu_memory = [r['gpu_memory_percent'] for r in results]\n",
    "    frame_completion = [r['frame_completion_rate'] for r in results]\n",
    "    frame_miss = [r['frame_miss_rate'] for r in results]\n",
    "    p99s = [r['p99_latency_ms'] for r in results]\n",
    "    mins = [r['min_latency_ms'] for r in results]\n",
    "    maxs = [r['max_latency_ms'] for r in results]\n",
    "    avg_frames_completed = [r['avg_frames_completed'] for r in results]\n",
    "    avg_frames_expected = [r['avg_frames_expected'] for r in results]\n",
    "    \n",
    "    # Normalize GPU utilization for color mapping\n",
    "    norm_gpu_util = np.array(gpu_utils) / 100.0\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Create scatter plot with GPU utilization as color\n",
    "    scatter = plt.scatter(\n",
    "        x_streams, y_latency,\n",
    "        c=norm_gpu_util,\n",
    "        cmap='viridis',\n",
    "        s=100,\n",
    "        marker='o',\n",
    "        edgecolor='k',\n",
    "        alpha=0.85\n",
    "    )\n",
    "    \n",
    "    # Find the best performance point\n",
    "    best_idx = None\n",
    "    max_streams = -1\n",
    "    \n",
    "    # First try to find point meeting all criteria\n",
    "    for i, (streams, latency, frame_comp, gpu_util) in enumerate(zip(x_streams, y_latency, frame_completion, gpu_utils)):\n",
    "        if (latency <= latency_cap_ms and \n",
    "            frame_comp >= efficiency_threshold and \n",
    "            gpu_util <= gpu_util_cap and \n",
    "            streams > max_streams):\n",
    "            best_idx = i\n",
    "            max_streams = streams\n",
    "    \n",
    "    # If no point meets all criteria, find best latency point under cap\n",
    "    if best_idx is None:\n",
    "        for i, (streams, latency) in enumerate(zip(x_streams, y_latency)):\n",
    "            if latency <= latency_cap_ms and streams > max_streams:\n",
    "                best_idx = i\n",
    "                max_streams = streams\n",
    "    \n",
    "    # Add annotation for best point\n",
    "    if best_idx is not None:\n",
    "        x = x_streams[best_idx]\n",
    "        y = y_latency[best_idx]\n",
    "        label_text = (\n",
    "            r\"$\\bf{Best\\ Performance\\ Point}$:\"\n",
    "            f\"\\nStreams: {x}\\n\"\n",
    "            f\"Avg Latency: {y_latency[best_idx]:.1f}ms\\n\"\n",
    "            f\"P99 Latency: {p99s[best_idx]:.1f}ms\\n\"\n",
    "            f\"Min Latency: {mins[best_idx]:.1f}ms\\n\"\n",
    "            f\"Max Latency: {maxs[best_idx]:.1f}ms\\n\"\n",
    "            f\"Frame Completion: {frame_completion[best_idx]:.1f}%\\n\"\n",
    "            f\"Frames Completed: {avg_frames_completed[best_idx]:.0f}/{avg_frames_expected[best_idx]:.0f} (avg)\\n\"\n",
    "            f\"Frame Miss Rate: {frame_miss[best_idx]:.1f}%\\n\"\n",
    "            f\"GPU Util: {gpu_utils[best_idx]:.1f}%\\n\"\n",
    "            f\"GPU Memory: {gpu_memory[best_idx]:.1f}%\"\n",
    "        )\n",
    "        # Highlight the best point with annotation\n",
    "        plt.annotate(\n",
    "            label_text,\n",
    "            (x, y),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(-160, 50),\n",
    "            ha='left',\n",
    "            va='bottom',\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"gray\", alpha=0.9),\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=-0.3\")\n",
    "        )\n",
    "    \n",
    "    # Add latency threshold line\n",
    "    plt.axhline(latency_cap_ms, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Latency Cap: {latency_cap_ms}ms')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel(f\"Concurrent Streams\", fontsize=10)\n",
    "    plt.ylabel(\"Average Latency (ms)\", fontsize=10)\n",
    "    plt.title(\"Performance Analysis: Latency vs Concurrent Streams\\n(Inference Processing Time)\", fontsize=12)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"GPU Utilization (%)\", fontsize=10)\n",
    "    cbar.ax.yaxis.set_tick_params(labelsize=10)\n",
    "    cbar.set_ticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    cbar.set_ticklabels(['0%', '25%', '50%', '75%', '100%'])\n",
    "    \n",
    "    plt.legend(fontsize=10, loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"{'Streams':<8} {'Avg Latency':<12} {'GPU Util':<10} {'GPU Memory':<12} {'Completion':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for r in results:\n",
    "        print(f\"{r['num_streams']:<8} {r['avg_latency_ms']:<12.1f} {r['gpu_util_percent']:<10.1f} {r['gpu_memory_percent']:<12.1f} {r['frame_completion_rate']:<12.1f}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee96ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your log file path\n",
    "log_file_path = \"/mnt/disk_e/Programming/real-time-object-detection/client-triton/client/logs/app.log\"\n",
    "\n",
    "plot_performance_analysis(\n",
    "    log_file_path=log_file_path,\n",
    "    latency_cap_ms=34,\n",
    "\n",
    "    # Thresholds for best point calculation\n",
    "    efficiency_threshold=90,\n",
    "    gpu_util_cap=90\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
